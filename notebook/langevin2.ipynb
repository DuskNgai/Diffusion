{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBDdp4F1IaHD"
      },
      "source": [
        "# Introduction to Langevin Dynamics\n",
        "\n",
        "In this notebook, we will walk through a simple demo for Langevin dynamics, where the goal is to sample from a distribution $p(x)$ using only its score function $\\nabla_{x} \\log p(x)$.\n",
        "Here we assume a toy setting where $p(x)$ is known.\n",
        "In most practical cases we only have access a dataset of samples $\\mathcal{D} = \\{x_0, x_1, \\ldots, x_n\\} \\sim p(x)$, in which case we might use a technique called score matching to estimate the score function [1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR-hK5T1ACBo"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVu-YP1Dnr-y"
      },
      "source": [
        "Here we define the log pdf and the gradient of the log pdf (i.e., the score function). We also provide a function for plotting the target shape corresponding for this specific elliptical logpdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWmvDcSAnqLj"
      },
      "outputs": [],
      "source": [
        "def logpdf(\n",
        "    x: torch.Tensor, \n",
        "    rx: float = 2.5, \n",
        "    ry: float = 2.5, \n",
        "    cx: float = 0.0, \n",
        "    cy: float = 0.0\n",
        ") -> torch.Tensor:\n",
        "    shifted_x = x - torch.tensor([cx, cy])\n",
        "    scaled_x = shifted_x / torch.tensor([rx, ry])\n",
        "    r = torch.norm(scaled_x, dim=-1)\n",
        "    return -(r - 1) ** 2 / 0.033\n",
        "\n",
        "def create_grad_func(\n",
        "    logpdf: Callable[..., torch.Tensor], \n",
        "    **kwargs: Any\n",
        ") -> Callable[[torch.Tensor], torch.Tensor]:\n",
        "    def grad_logpdf(x: torch.Tensor) -> torch.Tensor:\n",
        "        x.requires_grad_(True)\n",
        "        log_prob = logpdf(x, **kwargs)\n",
        "        return torch.autograd.grad(log_prob.sum(), x)[0]\n",
        "    return grad_logpdf\n",
        "\n",
        "def create_shape(\n",
        "    rx: float = 2.5, \n",
        "    ry: float = 2.5, \n",
        "    cx: float = 0.0, \n",
        "    cy: float = 0.0\n",
        ") -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"class\": \"Ellipse\",\n",
        "        \"kwargs\": {\n",
        "            \"width\": 2 * rx,\n",
        "            \"height\": 2 * ry,\n",
        "            \"xy\": (cx, cy)\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl4Zfd-8oDn8"
      },
      "source": [
        "Here we define some utility functions for visualizing results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H34Hz2G-oBhM"
      },
      "outputs": [],
      "source": [
        "def plot_frame(\n",
        "    particles: torch.Tensor, \n",
        "    step: int, \n",
        "    shape: Dict[str, Any], \n",
        "    figsize: Tuple[float, float] = (4, 4), \n",
        "    lim: Tuple[float, float] = (-3, 3)\n",
        ") -> np.ndarray:\n",
        "    particles_np = particles.detach().cpu().numpy()\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.scatter(particles_np[step, :, 0], particles_np[step, :, 1], alpha=0.1, s=1, color=\"blue\")\n",
        "    ax.set_xlim(*lim)\n",
        "    ax.set_ylim(*lim)\n",
        "    ax.set_xlabel(\"x coord\")\n",
        "    ax.set_ylabel(\"y coord\")\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.set_title(f\"Langevin Sampler at t = {step}\")\n",
        "\n",
        "    shape_cls = shape[\"class\"]\n",
        "    shape_patch = getattr(matplotlib.patches, shape_cls)(\n",
        "        edgecolor=\"red\",\n",
        "        facecolor=\"none\",\n",
        "        linewidth=2,\n",
        "        **shape[\"kwargs\"]\n",
        "    )\n",
        "    ax.add_patch(shape_patch)\n",
        "\n",
        "    fig.canvas.draw()\n",
        "    buf = fig.canvas.buffer_rgba()\n",
        "    image = np.asarray(buf)\n",
        "\n",
        "    plt.close()\n",
        "    return image\n",
        "\n",
        "def plot_trajectory(\n",
        "    particles: torch.Tensor, \n",
        "    particle_idx: int, \n",
        "    axis_names: List[str] = [\"x\", \"y\"], \n",
        "    figsize: Tuple[float, float] = (10, 3), \n",
        "    lim: Tuple[float, float] = (-3, 3)\n",
        ") -> None:\n",
        "    particles_np = particles.detach().cpu().numpy()\n",
        "    fig, ax = plt.subplots(1, len(axis_names), figsize=figsize)\n",
        "    for axis, axis_name in enumerate(axis_names):\n",
        "        trajectory = particles_np[:, particle_idx, axis]\n",
        "        ax[axis].plot(trajectory)\n",
        "        ax[axis].set_ylim(*lim)\n",
        "        ax[axis].set_title(f\"Trajectory of particle {particle_idx} along {axis_name}-axis\")\n",
        "        ax[axis].set_xlabel(\"timestep\")\n",
        "        ax[axis].set_ylabel(f\"{axis_names[axis]} coord\")\n",
        "\n",
        "def frames_to_image(frames: List[np.ndarray]) -> Image.Image:\n",
        "    w, h = frames[0].shape[1], frames[0].shape[0]\n",
        "    collated = Image.new(\"RGB\", (w * len(frames), h))\n",
        "    for i, frame in enumerate(frames):\n",
        "        collated.paste(Image.fromarray(frame), (i * w, 0))\n",
        "    return collated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXD7fhDgIzIe"
      },
      "source": [
        "### Part (a)\n",
        "\n",
        "**Finish the implementation of `langevin_update` and `sample_langevin`.**\n",
        "\n",
        "Recall that the update equation at timestep $t$ with step size $\\eta$ and random noise $\\epsilon \\sim \\mathcal{N}(0,I)$ is\n",
        "\n",
        "$x_{t+1} = x_t + \\eta\\nabla_x \\log p(x) + \\sqrt{2\\eta}\\epsilon$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6j2dsfTADoo"
      },
      "outputs": [],
      "source": [
        "def langevin_update(\n",
        "    grad_func: Callable[[torch.Tensor], torch.Tensor], \n",
        "    current_particles: torch.Tensor, \n",
        "    noise: torch.Tensor, \n",
        "    eta: float\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Runs a single Langevin update step on current_particles.\n",
        "    Returns a tensor of shape (num_particles, 2).\n",
        "    \"\"\"\n",
        "    next_particles = current_particles + eta * grad_func(current_particles) + (2 * eta) ** 0.5 * noise\n",
        "    return next_particles\n",
        "\n",
        "def sample_langevin(\n",
        "    grad_func: Callable[[torch.Tensor], torch.Tensor], \n",
        "    particles: torch.Tensor, \n",
        "    num_steps: int, \n",
        "    eta: float\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Takes randomly initialized particles and runs them through a Langevin sampler.\n",
        "    Returns a tensor of shape (num_steps, num_particles, 2).\n",
        "    \"\"\"\n",
        "    particles_over_time = [particles]\n",
        "    for _ in range(num_steps):\n",
        "        noise = torch.randn(particles.shape, device=particles.device)\n",
        "        particles = langevin_update(grad_func, particles, noise, eta)\n",
        "        particles_over_time.append(particles)\n",
        "    particles_over_time = torch.stack(particles_over_time)\n",
        "    return particles_over_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcCb7_5hJNUR"
      },
      "source": [
        "Now that you've completed your implementation, run the sampler and visualize results!\n",
        "\n",
        "For Langevin sampling, you can control the number of particles (`num_particles`), the dimension of each particle (`num_dims`), the number of update steps (`num_steps`), and the step size (`eta`). You can also control the shape of the base logpdf, e.g. the radii of the x and y axes of the ellipse (`rx` and `ry`) and the center (`cx` and `cy`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBmOxNfEseyM"
      },
      "outputs": [],
      "source": [
        "def sample_and_viz_langevin(\n",
        "    device: torch.device,\n",
        "    langevin_kwargs: Dict[str, Any],\n",
        "    ellipse_kwargs: Dict[str, Any],\n",
        "    init_particles: Optional[torch.Tensor] = None\n",
        ") -> List[np.ndarray]:\n",
        "    if init_particles is None:\n",
        "        # Initialize particles\n",
        "        init_particles = torch.randn(\n",
        "            langevin_kwargs[\"num_particles\"],\n",
        "            langevin_kwargs[\"num_dims\"],\n",
        "            device=device\n",
        "        )\n",
        "    # Run Langevin sampling\n",
        "    data = sample_langevin(\n",
        "        create_grad_func(logpdf, **ellipse_kwargs),\n",
        "        init_particles,\n",
        "        langevin_kwargs[\"num_steps\"],\n",
        "        langevin_kwargs[\"eta\"]\n",
        "    )\n",
        "    # Plot results\n",
        "    frames = []\n",
        "    for t in tqdm(range(data.shape[0])):\n",
        "        frames.append(plot_frame(data, t, create_shape(**ellipse_kwargs)))\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8X67LabsiaE"
      },
      "source": [
        "First, run sampling with the default hyperparameters.\n",
        "\n",
        "*Note:* To simplify the runtime and plotting, throughout this problem you will only run the Langevin sampler for a few iterations. In practice, however, you would typically run the sampler for longer (e.g., several thousand iterations), to ensure the Markov chain has converged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPcMOGq-_Xr3"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "langevin_kwargs = {\n",
        "    \"num_particles\": 10000,\n",
        "    \"num_dims\": 2,\n",
        "    \"num_steps\": 10,\n",
        "    \"eta\": torch.tensor([1e-2, 1e-2])\n",
        "}\n",
        "ellipse_kwargs = {\n",
        "    \"rx\": 1.5,\n",
        "    \"ry\": 1.5,\n",
        "    \"cx\": 0.0,\n",
        "    \"cy\": 0.0\n",
        "}\n",
        "\n",
        "frames = sample_and_viz_langevin(device, langevin_kwargs, ellipse_kwargs)\n",
        "frames_to_image(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0__nOvedtJ2i"
      },
      "source": [
        "Now let\"s adjust the radius of the elliptical logpdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M408Zh_UtFjc"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "langevin_kwargs = {\n",
        "    \"num_particles\": 10000,\n",
        "    \"num_dims\": 2,\n",
        "    \"num_steps\": 10,\n",
        "    \"eta\": torch.tensor([1e-2, 1e-2])\n",
        "}\n",
        "ellipse_kwargs = {\n",
        "    \"rx\": 1.0,\n",
        "    \"ry\": 2.5,\n",
        "    \"cx\": 0.0,\n",
        "    \"cy\": 0.0\n",
        "}\n",
        "\n",
        "frames = sample_and_viz_langevin(device, langevin_kwargs, ellipse_kwargs)\n",
        "frames_to_image(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XgLKQrotR7V"
      },
      "source": [
        "### Part (b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE25qGHY1B0F"
      },
      "source": [
        "Let\"s see if we can get a better fit to the elliptical logpdf in (c) by tuning each dimension of `eta`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbkVhhTE0Pxt"
      },
      "outputs": [],
      "source": [
        "eta = torch.tensor([1e-2, 1e-1])\n",
        "\n",
        "device = \"cpu\"\n",
        "langevin_kwargs = {\n",
        "    \"num_particles\": 10000,\n",
        "    \"num_dims\": 2,\n",
        "    \"num_steps\": 10,\n",
        "    \"eta\": eta\n",
        "}\n",
        "ellipse_kwargs = {\n",
        "    \"rx\": 1.0,\n",
        "    \"ry\": 2.5,\n",
        "    \"cx\": 0.0,\n",
        "    \"cy\": 0.0\n",
        "}\n",
        "\n",
        "frames = sample_and_viz_langevin(device, langevin_kwargs, ellipse_kwargs)\n",
        "frames_to_image(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZGKv4h-t2y7"
      },
      "source": [
        "### Part (c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu035aUF1E6J"
      },
      "source": [
        "Here let\"s move the center of the logpdf away from the origin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAZZhCmlt4Ry"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "langevin_kwargs = {\n",
        "    \"num_particles\": 10000,\n",
        "    \"num_dims\": 2,\n",
        "    \"num_steps\": 10,\n",
        "    \"eta\": torch.tensor([1e-2, 1e-2])\n",
        "}\n",
        "ellipse_kwargs = {\n",
        "    \"rx\": 1.5,\n",
        "    \"ry\": 1.5,\n",
        "    \"cx\": 1.0,\n",
        "    \"cy\": 1.0\n",
        "}\n",
        "\n",
        "frames = sample_and_viz_langevin(device, langevin_kwargs, ellipse_kwargs)\n",
        "frames_to_image(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v0GHRY_tGCY"
      },
      "source": [
        "For the off-centered logpdf in (e), let\"s try to get a better fit by tuning the initialization `init_particles`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsdnF82I5lOn"
      },
      "outputs": [],
      "source": [
        "init_particles = torch.randn(\n",
        "    langevin_kwargs[\"num_particles\"],\n",
        "    langevin_kwargs[\"num_dims\"],\n",
        "    device=device\n",
        ") * 1.5 + torch.tensor([1.0, 1.0], device=device)\n",
        "\n",
        "device = \"cpu\"\n",
        "langevin_kwargs = {\n",
        "    \"num_particles\": 10000,\n",
        "    \"num_dims\": 2,\n",
        "    \"num_steps\": 10,\n",
        "    \"eta\": torch.tensor([1e-2, 1e-2])\n",
        "}\n",
        "ellipse_kwargs = {\n",
        "    \"rx\": 1.5,\n",
        "    \"ry\": 1.5,\n",
        "    \"cx\": 1.0,\n",
        "    \"cy\": 1.0\n",
        "}\n",
        "\n",
        "frames = sample_and_viz_langevin(device, langevin_kwargs, ellipse_kwargs, init_particles=init_particles)\n",
        "frames_to_image(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agJfSgGvMDE1"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "That's it! Congratulations on finishing the notebook.\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS 2019.\n",
        "\n",
        "[2] Shreyas Kapur. Code Focused Guide on Score-Based Image Models. Blog Post 2023.\n",
        "\n",
        "[3] Yang Song. Generative Modeling by Estimating Gradients of the Data Distribution. Blog Post. Blog Post 2021."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
