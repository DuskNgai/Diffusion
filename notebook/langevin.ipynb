{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "os.environ[\"HF_HOME\"] = str(Path.cwd().joinpath(\"cache\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Langevin Dynamics by Gaussian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from gaussian import GaussianModel\n",
    "from gmm import GaussianMixtureModel\n",
    "from langevin import (\n",
    "    LangevinGaussianModelPipeline,\n",
    "    LangevinNoiseScheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_inference_result(\n",
    "    pipeline: LangevinNoiseScheduler,\n",
    "    batch_size: int,\n",
    "    num_inference_steps: int,\n",
    "    seed: int\n",
    "):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    algorithm_types = [\"ode\", \"sde\"]\n",
    "    random_samplings = [True, False]\n",
    "\n",
    "    for row_id, algorithm_type in enumerate(algorithm_types):\n",
    "        pipeline.scheduler.config.algorithm_type = algorithm_type\n",
    "        for col_id, random_sampling in enumerate(random_samplings):\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed(seed)\n",
    "\n",
    "            samples = pipeline(batch_size=batch_size, num_inference_steps=num_inference_steps, random_sampling=random_sampling)\n",
    "            samples = samples.detach().cpu().numpy()\n",
    "            mean, std = samples.mean(axis=0), samples.std(axis=0)\n",
    "            print(f\"{algorithm_type} {random_sampling}: mean={mean}, std={std}\")\n",
    "\n",
    "            plt.subplot(len(algorithm_types), len(random_samplings), row_id * len(random_samplings) + col_id + 1)\n",
    "            plt.hist2d(samples[:, 0], samples[:, 1], bins=[128, 128], range=[[-4, 4], [-4, 4]], cmap=\"plasma\", cmin=1)\n",
    "            plt.title(f\"{algorithm_type} {random_sampling}\")\n",
    "            plt.axis(\"equal\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "batch_size = 16384\n",
    "num_inference_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = LangevinGaussianModelPipeline(\n",
    "    GaussianModel(\n",
    "        mu=torch.tensor([0.0, 0.0]),\n",
    "        cov=torch.tensor([[1.0, 0.5], [0.5, 1.0]])\n",
    "    ),\n",
    "    LangevinNoiseScheduler(\n",
    "        t_min=0.0,\n",
    "        t_max=1.0,\n",
    "        algorithm_type=\"sde\",\n",
    "    )\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    pipeline = pipeline.to(device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_inference_result(pipeline, batch_size, num_inference_steps, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = LangevinGaussianModelPipeline(\n",
    "    GaussianMixtureModel(\n",
    "        num_groups_per_model=8,\n",
    "        num_gs_per_group=8,\n",
    "        model_radius=2,\n",
    "        group_radius=0.4,\n",
    "        sigma=0.04\n",
    "    ),\n",
    "    LangevinNoiseScheduler(\n",
    "        t_min=0.0,\n",
    "        t_max=1.0,\n",
    "        algorithm_type=\"sde\",\n",
    "    )\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    pipeline = pipeline.to(device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_inference_result(pipeline, batch_size, num_inference_steps, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
